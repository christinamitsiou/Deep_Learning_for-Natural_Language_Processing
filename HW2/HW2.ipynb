{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963ab023",
   "metadata": {
    "papermill": {
     "duration": 0.007158,
     "end_time": "2025-05-05T22:32:04.201430",
     "exception": false,
     "start_time": "2025-05-05T22:32:04.194272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d45ba",
   "metadata": {
    "papermill": {
     "duration": 0.005531,
     "end_time": "2025-05-05T22:32:04.212876",
     "exception": false,
     "start_time": "2025-05-05T22:32:04.207345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **1. Imports, Loading the Datasets, Initializing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c3dc9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:32:04.226677Z",
     "iopub.status.busy": "2025-05-05T22:32:04.226358Z",
     "iopub.status.idle": "2025-05-05T22:37:21.678351Z",
     "shell.execute_reply": "2025-05-05T22:37:21.677346Z"
    },
    "papermill": {
     "duration": 328.486439,
     "end_time": "2025-05-05T22:37:32.704974",
     "exception": false,
     "start_time": "2025-05-05T22:32:04.218535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\r\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Collecting textsearch>=0.0.21 (from contractions)\r\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\r\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\r\n",
      "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\r\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\r\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\r\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189385</td>\n",
       "      <td>@whoisralphie dude  I'm so bummed ur leaving!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58036</td>\n",
       "      <td>oh my god, a severed foot was foun in a wheely...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190139</td>\n",
       "      <td>I end up &amp;quot;dog dialing&amp;quot; sumtimes. Wha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99313</td>\n",
       "      <td>@_rachelx meeeee toooooo!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157825</td>\n",
       "      <td>I was hoping I could stay home and work today,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               Text  Label\n",
       "0  189385      @whoisralphie dude  I'm so bummed ur leaving!      0\n",
       "1   58036  oh my god, a severed foot was foun in a wheely...      0\n",
       "2  190139  I end up &quot;dog dialing&quot; sumtimes. Wha...      1\n",
       "3   99313                         @_rachelx meeeee toooooo!       0\n",
       "4  157825  I was hoping I could stay home and work today,...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install contractions\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import nltk\n",
    "import html\n",
    "import emoji\n",
    "import contractions\n",
    "import random\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the Word2Vec model\n",
    "w2v = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "# Load the data\n",
    "train_data      = pd.read_csv(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/train_dataset.csv\")\n",
    "validation_data = pd.read_csv(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/val_dataset.csv\")\n",
    "test_data       = pd.read_csv(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/test_dataset.csv\")\n",
    "\n",
    "# FOR TESTING\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce024d2",
   "metadata": {
    "papermill": {
     "duration": 10.796995,
     "end_time": "2025-05-05T22:37:54.480714",
     "exception": false,
     "start_time": "2025-05-05T22:37:43.683719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448dc8a4",
   "metadata": {
    "papermill": {
     "duration": 10.957371,
     "end_time": "2025-05-05T22:38:16.350932",
     "exception": false,
     "start_time": "2025-05-05T22:38:05.393561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **2. Text Preprocessing / Data cleaning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8890d",
   "metadata": {
    "papermill": {
     "duration": 10.864728,
     "end_time": "2025-05-05T22:38:38.151119",
     "exception": false,
     "start_time": "2025-05-05T22:38:27.286391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `preprocess_text` function cleans and preprocess the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5f70be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:38:59.764588Z",
     "iopub.status.busy": "2025-05-05T22:38:59.764228Z",
     "iopub.status.idle": "2025-05-05T22:38:59.771569Z",
     "shell.execute_reply": "2025-05-05T22:38:59.770783Z"
    },
    "papermill": {
     "duration": 10.969179,
     "end_time": "2025-05-05T22:38:59.773008",
     "exception": false,
     "start_time": "2025-05-05T22:38:48.803829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # 0. Drop empty tweets\n",
    "    if not text or not text.strip():\n",
    "        return None\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # 3. Tag hashtags as both token and marker\n",
    "    text = re.sub(r\"#(\\w+)\", r\"<HASHTAG_\\1> \\1\", text)\n",
    "\n",
    "    # 4. Anonymize emails\n",
    "    text = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\",\"<EMAIL>\",text)\n",
    "\n",
    "    # 5. Anonymize phone numbers\n",
    "    text = re.sub(r\"\\+?\\d[\\d\\-\\s]{7,}\\d\",\"<PHONE>\",text)\n",
    "    \n",
    "    # 6. Tokenize into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 7. Drop empty tokens\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "\n",
    "    # 8. Lemmatize each token\n",
    "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "\n",
    "    # 9. OOV handling\n",
    "    tokens = [tok if tok in w2v.key_to_index else \"<UNK>\" for tok in tokens]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbf2f3",
   "metadata": {
    "papermill": {
     "duration": 10.818592,
     "end_time": "2025-05-05T22:39:21.492329",
     "exception": false,
     "start_time": "2025-05-05T22:39:10.673737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Greedy Selection Funcion**\n",
    "A greedy pipeline builder that:\n",
    "\n",
    "1. Starts with a minimal preprocessing setup.\n",
    "2. Adds one preprocessing step at a time.\n",
    "3. After each addition, runs a quick training/validation check.\n",
    "4. Keeps the new step only if it improves the validation score.\n",
    "5. Repeats until no further improvements are found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2777833",
   "metadata": {
    "papermill": {
     "duration": 10.713327,
     "end_time": "2025-05-05T22:39:43.068889",
     "exception": false,
     "start_time": "2025-05-05T22:39:32.355562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **3. Classes and Functions Definitions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9106bd",
   "metadata": {
    "papermill": {
     "duration": 10.771026,
     "end_time": "2025-05-05T22:40:04.595734",
     "exception": false,
     "start_time": "2025-05-05T22:39:53.824708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Define all the classes and functions needed for the next steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2b6c4",
   "metadata": {
    "papermill": {
     "duration": 10.768709,
     "end_time": "2025-05-05T22:40:26.229269",
     "exception": false,
     "start_time": "2025-05-05T22:40:15.460560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A two-layer `PyTorch` model that:\n",
    "\n",
    "1. Maps the 300-dimention input to a hidden layer.\n",
    "2. Applies ReLU activation and dropout.\n",
    "3. Passes through a second hidden layer with ReLU and dropout.\n",
    "4. Outputs a single logit for sentiment prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216ff32e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:40:47.864320Z",
     "iopub.status.busy": "2025-05-05T22:40:47.863978Z",
     "iopub.status.idle": "2025-05-05T22:40:47.870612Z",
     "shell.execute_reply": "2025-05-05T22:40:47.869847Z"
    },
    "papermill": {
     "duration": 10.878893,
     "end_time": "2025-05-05T22:40:47.872167",
     "exception": false,
     "start_time": "2025-05-05T22:40:36.993274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleSentimentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, dropout=0.0):\n",
    "        super(SimpleSentimentModel, self).__init__()\n",
    "        \n",
    "        # First layer that connects inputs to hidden layer\n",
    "        self.input_to_hidden = nn.Linear(input_size, hidden_size) \n",
    "        \n",
    "        # ReLU activation to makw the model learn better\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout after first hidden layer\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "        # Second hidden layer (connects the first hidden layer to the second)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Dropout after second hidden layer\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer\n",
    "        self.hidden_to_output = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_to_hidden(x)  # First layer\n",
    "        x = self.relu(x)             # Activation function\n",
    "        x = self.dropout1(x)         # Dropout\n",
    "        \n",
    "        x = self.hidden_to_hidden(x) # Second layer\n",
    "        x = self.relu(x)             # Activation function again\n",
    "        x = self.dropout2(x)         # Dropout\n",
    "        \n",
    "        return self.hidden_to_output(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92a30e",
   "metadata": {
    "papermill": {
     "duration": 10.811563,
     "end_time": "2025-05-05T22:41:09.463144",
     "exception": false,
     "start_time": "2025-05-05T22:40:58.651581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A PyTorch `Dataset` that:\n",
    "\n",
    "1. Stores tweet feature vectors and labels as tensors.\n",
    "2. `__len__`: Returns the number of samples.\n",
    "3. `__getitem__`: Retrieves the `(features, label)` pair by index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0110170b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:41:31.132096Z",
     "iopub.status.busy": "2025-05-05T22:41:31.131725Z",
     "iopub.status.idle": "2025-05-05T22:41:31.137177Z",
     "shell.execute_reply": "2025-05-05T22:41:31.136410Z"
    },
    "papermill": {
     "duration": 10.808264,
     "end_time": "2025-05-05T22:41:31.138678",
     "exception": false,
     "start_time": "2025-05-05T22:41:20.330414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels):\n",
    "        \n",
    "        # Convert tweet vectors to tensor\n",
    "        self.tweets = torch.tensor(tweets, dtype=torch.float32)  \n",
    "         # Convert labels (0 or 1) to tensor\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tweets[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be32d1",
   "metadata": {
    "papermill": {
     "duration": 10.926429,
     "end_time": "2025-05-05T22:41:52.835933",
     "exception": false,
     "start_time": "2025-05-05T22:41:41.909504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A function to convert each tweet into the average of its word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6736c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:42:14.202569Z",
     "iopub.status.busy": "2025-05-05T22:42:14.202239Z",
     "iopub.status.idle": "2025-05-05T22:42:14.207159Z",
     "shell.execute_reply": "2025-05-05T22:42:14.206420Z"
    },
    "papermill": {
     "duration": 10.576542,
     "end_time": "2025-05-05T22:42:14.208650",
     "exception": false,
     "start_time": "2025-05-05T22:42:03.632108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_tweet(tokens, w2v_model, dim=300):\n",
    "    vectors = [w2v_model[word] for word in tokens if word in w2v_model]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947f83a",
   "metadata": {
    "papermill": {
     "duration": 10.7701,
     "end_time": "2025-05-05T22:42:35.940416",
     "exception": false,
     "start_time": "2025-05-05T22:42:25.170316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A helper function that takes lists of training losses, validation losses, and validation accuracies, then:\n",
    "\n",
    "1. Plots and saves the training vs. validation loss curve.\n",
    "2. Plots and saves the validation accuracy curve.\n",
    "\n",
    "The `prefix` argument customizes the plot titles and filenames.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e52b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:42:57.395804Z",
     "iopub.status.busy": "2025-05-05T22:42:57.395229Z",
     "iopub.status.idle": "2025-05-05T22:42:57.401809Z",
     "shell.execute_reply": "2025-05-05T22:42:57.400935Z"
    },
    "papermill": {
     "duration": 10.879713,
     "end_time": "2025-05-05T22:42:57.403368",
     "exception": false,
     "start_time": "2025-05-05T22:42:46.523655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_learning_curves(train_losses, val_losses, val_accs, prefix=\"run\"):\n",
    "\n",
    "    epochs = len(train_losses)\n",
    "    x = range(1, epochs + 1)\n",
    "\n",
    "    # Loss curve\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(x, val_losses,   label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{prefix} Learning Curve – Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{prefix.lower()}_loss_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy curve\n",
    "    plt.figure()\n",
    "    plt.plot(x, val_accs, label=\"Val Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(f\"{prefix} Learning Curve – Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{prefix.lower()}_accuracy_curve.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c7166",
   "metadata": {
    "papermill": {
     "duration": 10.766728,
     "end_time": "2025-05-05T22:43:18.947861",
     "exception": false,
     "start_time": "2025-05-05T22:43:08.181133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Greedy Selection Function used for testing\n",
    "\n",
    "Commented out for sumbission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad608bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:43:40.588807Z",
     "iopub.status.busy": "2025-05-05T22:43:40.587871Z",
     "iopub.status.idle": "2025-05-05T22:43:40.594646Z",
     "shell.execute_reply": "2025-05-05T22:43:40.594008Z"
    },
    "papermill": {
     "duration": 10.774755,
     "end_time": "2025-05-05T22:43:40.596010",
     "exception": false,
     "start_time": "2025-05-05T22:43:29.821255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# def spell_correct(text):\n",
    "#     try:\n",
    "#         corrected = str(TextBlob(text).correct())\n",
    "#     except Exception:\n",
    "#         corrected = text  \n",
    "#     return corrected\n",
    "\n",
    "# english_vocab = set(words.words())\n",
    "\n",
    "# def is_english_word(word):\n",
    "#     return word.lower() in english_vocab\n",
    "\n",
    "# # Define each preprocessing step as a function\n",
    "# def drop_empty(text):\n",
    "#     return text if text and text.strip() else None\n",
    "\n",
    "# def lowercase(text):\n",
    "#     return text.lower()\n",
    "\n",
    "# def expand_contractions(text):\n",
    "#     return contractions.fix(text)\n",
    "\n",
    "# def html_unescape_step(text):\n",
    "#     return html.unescape(text)\n",
    "\n",
    "# def anonymize_url(text):\n",
    "#     return re.sub(r\"https?://\\S+\", \"<URL>\", text)\n",
    "\n",
    "# def anonymize_email(text):\n",
    "#     return re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \"<EMAIL>\", text)\n",
    "\n",
    "# def anonymize_phone(text):\n",
    "#     return re.sub(r\"\\+?\\d[\\d\\-\\s]{7,}\\d\", \"<PHONE>\", text)\n",
    "\n",
    "# def tokenize(text):\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "# def remove_empty_tokens(tokens):\n",
    "#     return [tok for tok in tokens if tok]\n",
    "\n",
    "# def spell_correct_step(tokens):\n",
    "#     return [spell_correct(tok) for tok in tokens]\n",
    "\n",
    "# def lemmatize_step(tokens):\n",
    "#     return [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "\n",
    "# def remove_stopwords_step(tokens):\n",
    "#     return [tok for tok in tokens if tok not in stop_words]\n",
    "\n",
    "# def english_filter_step(tokens):\n",
    "#     return [tok for tok in tokens if is_english_word(tok, threshold=2.0)]\n",
    "\n",
    "# # Apply each pipeline\n",
    "# def apply_pipeline(text, pipeline):\n",
    "#     data = text\n",
    "#     for step in pipeline:\n",
    "#         data = step(data)\n",
    "#         if data is None:\n",
    "#             return None\n",
    "#     return data\n",
    "\n",
    "# # Train 3 epochs and return validation accuracy \n",
    "# def run_experiment(pipeline):\n",
    "    \n",
    "#     # Preprocess & vectorize the train set \n",
    "#     X_train, y_train = [], []\n",
    "#     for txt, lbl in zip(train_data[\"Text\"], train_data[\"Label\"]):\n",
    "#         tokens = apply_pipeline(txt, pipeline)\n",
    "#         if tokens is None: continue\n",
    "#         X_train.append(vectorize_tweet(tokens, w2v))\n",
    "#         y_train.append(lbl)\n",
    "#     X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "#     train_loader = DataLoader(TweetDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    \n",
    "#     # Preprocess & vectorize the validation set \n",
    "#     X_val, y_val = [], []\n",
    "#     for txt, lbl in zip(validation_data[\"Text\"], validation_data[\"Label\"]):\n",
    "#         tokens = apply_pipeline(txt, pipeline)\n",
    "#         if tokens is None: continue\n",
    "#         X_val.append(vectorize_tweet(tokens, w2v))\n",
    "#         y_val.append(lbl)\n",
    "#     X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "#     val_loader = DataLoader(TweetDataset(X_val, y_val), batch_size=64)\n",
    "    \n",
    "#     # Set up the model,loss,optimazer\n",
    "#     model = SimpleSentimentModel(input_size=300, hidden_size=128)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "#     # Train for 3 epochs \n",
    "#     model.train()\n",
    "#     for epoch in range(3):\n",
    "#         total_loss = 0.0\n",
    "#         for vecs, labels in train_loader:\n",
    "#             preds = model(vecs).squeeze()\n",
    "#             loss  = criterion(preds, labels)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#         print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "#     # Evaluate on validation \n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for vecs, labels in val_loader:\n",
    "#             preds = (model(vecs).squeeze() > 0.5).float()\n",
    "#             correct += (preds == labels).sum().item()\n",
    "#             total   += len(labels)\n",
    "#     acc = 100 * correct / total\n",
    "#     print(f\"Validation Accuracy: {acc:.2f}%\\n\")\n",
    "#     return acc\n",
    "\n",
    "# # Greedy selection, add each step one by one and if something increases the score, keep it\n",
    "\n",
    "# # Start with the minimal pipeline:\n",
    "# best_pipeline = [drop_empty, lowercase]\n",
    "# best_acc = run_experiment(best_pipeline)\n",
    "# print(f\"Baseline ({[fn.__name__ for fn in best_pipeline]}): {best_acc:.2f}%\\n\")\n",
    "\n",
    "# # Candidate steps \n",
    "# candidates = [\n",
    "#     expand_contractions,\n",
    "#     html_unescape_step,\n",
    "#     anonymize_url,\n",
    "#     anonymize_email,\n",
    "#     anonymize_phone,\n",
    "#     tokenize,\n",
    "#     remove_empty_tokens,\n",
    "#     spell_correct_step,\n",
    "#     lemmatize_step,\n",
    "#     remove_stopwords_step,\n",
    "#     english_filter_step,\n",
    "# ]\n",
    "\n",
    "# # Try adding each step one by one\n",
    "# for step in candidates:\n",
    "#     trial = best_pipeline + [step]\n",
    "#     acc = run_experiment(trial)\n",
    "#     print(f\"Trial + {step.__name__:<20} → {acc:.2f}%\")\n",
    "#     if acc > best_acc:\n",
    "#         best_acc      = acc\n",
    "#         best_pipeline = trial\n",
    "#         print(f\"  → Keeping {step.__name__} (new best: {best_acc:.2f}%)\\n\")\n",
    "\n",
    "# # Final pipeline\n",
    "# print(\"Final pipeline order:\")\n",
    "# for fn in best_pipeline:\n",
    "#     print(\" -\", fn.__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2913753",
   "metadata": {
    "papermill": {
     "duration": 10.777353,
     "end_time": "2025-05-05T22:44:02.130878",
     "exception": false,
     "start_time": "2025-05-05T22:43:51.353525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **4. Word Embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b53d97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:44:23.602108Z",
     "iopub.status.busy": "2025-05-05T22:44:23.601734Z",
     "iopub.status.idle": "2025-05-05T22:45:38.598919Z",
     "shell.execute_reply": "2025-05-05T22:45:38.597656Z"
    },
    "papermill": {
     "duration": 85.740667,
     "end_time": "2025-05-05T22:45:38.600923",
     "exception": false,
     "start_time": "2025-05-05T22:44:12.860256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Vectorize the train tweets\n",
    "train_vectors = np.array([vectorize_tweet(preprocess_text(text), w2v) for text in train_data[\"Text\"]])\n",
    "\n",
    "# Save to use later\n",
    "np.save(\"train_vectors.npy\", train_vectors)\n",
    "\n",
    "train_vectors = np.load(\"train_vectors.npy\")\n",
    "\n",
    "# Vectorize the validation tweets\n",
    "val_vectors   = np.array([vectorize_tweet(preprocess_text(text), w2v) for text in validation_data[\"Text\"]])\n",
    "np.save(\"val_vectors.npy\", val_vectors)\n",
    "\n",
    "val_vectors = np.load(\"val_vectors.npy\")\n",
    "\n",
    "# Vectorize the test tweets\n",
    "test_vectors  = np.array([vectorize_tweet(preprocess_text(text), w2v) for text in test_data[\"Text\"]])\n",
    "np.save(\"test_vectors.npy\", test_vectors)\n",
    "\n",
    "test_vectors = np.load(\"test_vectors.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f2536",
   "metadata": {
    "papermill": {
     "duration": 10.759864,
     "end_time": "2025-05-05T22:46:00.249246",
     "exception": false,
     "start_time": "2025-05-05T22:45:49.489382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set the model hyperparameters:\n",
    "- 300-dimensional input vectors\n",
    "- a 128-neuron hidden layer\n",
    "- 20% dropout\n",
    "Then create an instance of `SimpleSentimentModel` with those settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e542d3ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:46:22.089285Z",
     "iopub.status.busy": "2025-05-05T22:46:22.088956Z",
     "iopub.status.idle": "2025-05-05T22:46:22.129410Z",
     "shell.execute_reply": "2025-05-05T22:46:22.128427Z"
    },
    "papermill": {
     "duration": 11.140329,
     "end_time": "2025-05-05T22:46:22.131151",
     "exception": false,
     "start_time": "2025-05-05T22:46:10.990822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Default size of the tweet vector \n",
    "input_size = 300  \n",
    "# Default number of neurons in the hidden layer \n",
    "hidden_size = 128 \n",
    "# Default dropout rate\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Define the model\n",
    "model = SimpleSentimentModel(\n",
    "    input_size, \n",
    "    hidden_size, \n",
    "    output_size=1, \n",
    "    dropout=dropout_rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6d061",
   "metadata": {
    "papermill": {
     "duration": 14.786397,
     "end_time": "2025-05-05T22:46:49.411494",
     "exception": false,
     "start_time": "2025-05-05T22:46:34.625097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Move model to GPU if it is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32430c8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:47:11.520033Z",
     "iopub.status.busy": "2025-05-05T22:47:11.519116Z",
     "iopub.status.idle": "2025-05-05T22:47:11.547361Z",
     "shell.execute_reply": "2025-05-05T22:47:11.546600Z"
    },
    "papermill": {
     "duration": 11.057012,
     "end_time": "2025-05-05T22:47:11.549029",
     "exception": false,
     "start_time": "2025-05-05T22:47:00.492017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSentimentModel(\n",
       "  (input_to_hidden): Linear(in_features=300, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (hidden_to_hidden): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (hidden_to_output): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18be96",
   "metadata": {
    "papermill": {
     "duration": 10.710664,
     "end_time": "2025-05-05T22:47:33.305700",
     "exception": false,
     "start_time": "2025-05-05T22:47:22.595036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use binary cross-entropy with logits as our loss (`BCEWithLogitsLoss`) and the Adam optimizer with a learning rate of 0.001 to train the model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08cb426a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:47:55.334614Z",
     "iopub.status.busy": "2025-05-05T22:47:55.334252Z",
     "iopub.status.idle": "2025-05-05T22:48:01.400548Z",
     "shell.execute_reply": "2025-05-05T22:48:01.399592Z"
    },
    "papermill": {
     "duration": 17.016767,
     "end_time": "2025-05-05T22:48:01.402677",
     "exception": false,
     "start_time": "2025-05-05T22:47:44.385910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss function: Binary Cross-Entropy loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer: Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772a0f5",
   "metadata": {
    "papermill": {
     "duration": 11.03209,
     "end_time": "2025-05-05T22:48:23.702322",
     "exception": false,
     "start_time": "2025-05-05T22:48:12.670232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Wrap the processed tweet vectors and their labels in `TweetDataset`, then create `DataLoader`s with a batch size of 64—shuffling only the training loader to mix up examples each epoch.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208ded49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:48:45.887549Z",
     "iopub.status.busy": "2025-05-05T22:48:45.886495Z",
     "iopub.status.idle": "2025-05-05T22:48:46.016637Z",
     "shell.execute_reply": "2025-05-05T22:48:46.015818Z"
    },
    "papermill": {
     "duration": 11.280243,
     "end_time": "2025-05-05T22:48:46.018393",
     "exception": false,
     "start_time": "2025-05-05T22:48:34.738150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = train_data['Label'].values \n",
    "val_labels = validation_data['Label'].values\n",
    "\n",
    "train_dataset = TweetDataset(train_vectors, train_labels)\n",
    "val_dataset = TweetDataset(val_vectors, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf307d",
   "metadata": {
    "papermill": {
     "duration": 10.889672,
     "end_time": "2025-05-05T22:49:08.197050",
     "exception": false,
     "start_time": "2025-05-05T22:48:57.307378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A combined training and validation loop that, for each epoch:\n",
    "1. Trains the model on all batches in `train_loader`, tracking the average training loss.\n",
    "2. Evaluates on `val_loader` without gradient updates, computing validation loss and accuracy.\n",
    "3. Prints and returns lists of train losses, validation losses, and validation accuracies over all epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ee1ede3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:49:30.647477Z",
     "iopub.status.busy": "2025-05-05T22:49:30.646973Z",
     "iopub.status.idle": "2025-05-05T22:49:30.653545Z",
     "shell.execute_reply": "2025-05-05T22:49:30.652572Z"
    },
    "papermill": {
     "duration": 11.554332,
     "end_time": "2025-05-05T22:49:30.655211",
     "exception": false,
     "start_time": "2025-05-05T22:49:19.100879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, epochs):\n",
    "    \n",
    "#     train_losses, val_losses, val_accs = [], [], []    \n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "        \n",
    "#         # Training phase\n",
    "\n",
    "#         # Set the model to training mode\n",
    "#         model.train() \n",
    "#         running_loss = 0.0\n",
    "    \n",
    "#         for texts, labels in train_loader:\n",
    "#             # Move data to device\n",
    "#             texts, labels = texts.to(device), labels.to(device)\n",
    "    \n",
    "#             # Forward pass: get raw logits\n",
    "#             logits = model(texts)\n",
    "    \n",
    "#             # Calculate loss on logits vs. labels\n",
    "#             loss = criterion(logits.squeeze(), labels)\n",
    "    \n",
    "#             # Backward pass: calculate gradients and update parameters\n",
    "\n",
    "#             # Clear previous gradients\n",
    "#             optimizer.zero_grad()  \n",
    "#             # Calculate new gradients\n",
    "#             loss.backward()        \n",
    "#             # Update model parameters\n",
    "#             optimizer.step()       \n",
    "\n",
    "#             # Keep track of the loss\n",
    "#             running_loss += loss.item()  \n",
    "    \n",
    "#         # Compute and record average training loss\n",
    "#         avg_train_loss = running_loss / len(train_loader)\n",
    "#         train_losses.append(avg_train_loss)\n",
    "    \n",
    "#         # Validation phase\n",
    "\n",
    "#         # Set the model to evaluation mode\n",
    "#         model.eval() \n",
    "#         running_val_loss = 0.0\n",
    "#         correct, total = 0, 0\n",
    "    \n",
    "#         with torch.no_grad():  \n",
    "#             for texts, labels in val_loader:\n",
    "#                 # Move data to device\n",
    "#                 texts, labels = texts.to(device), labels.to(device)\n",
    "    \n",
    "#                 # Forward pass: get raw logits\n",
    "#                 logits = model(texts).squeeze()\n",
    "    \n",
    "#                 # Calculate validation loss\n",
    "#                 running_val_loss += criterion(logits, labels).item()\n",
    "    \n",
    "#                 # Classification: sigmoid + threshold\n",
    "#                 probs = torch.sigmoid(logits)\n",
    "#                 predicted = (probs > 0.5).float()\n",
    "    \n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#         # Compute and record average validation loss and accuracy\n",
    "#         avg_val_loss = running_val_loss / len(val_loader)\n",
    "#         val_losses.append(avg_val_loss)\n",
    "    \n",
    "#         val_acc = 100 * correct / total\n",
    "#         val_accs.append(val_acc)\n",
    "    \n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{epochs}, \"\n",
    "#             f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "#             f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "#             f\"Val Acc: {val_acc:.2f}%\"\n",
    "#         )\n",
    "#     return train_losses, val_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86e3d226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:49:52.441005Z",
     "iopub.status.busy": "2025-05-05T22:49:52.440620Z",
     "iopub.status.idle": "2025-05-05T22:49:52.445245Z",
     "shell.execute_reply": "2025-05-05T22:49:52.444411Z"
    },
    "papermill": {
     "duration": 10.724792,
     "end_time": "2025-05-05T22:49:52.446613",
     "exception": false,
     "start_time": "2025-05-05T22:49:41.721821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Define the learning_rates ---\n",
    "# learning_rates = [1e-4, 5e-4, 1e-3]\n",
    "# base_hparams = {\n",
    "#     'hidden_size': 128,\n",
    "#     'dropout':     0.2,\n",
    "#     'batch_size':  64,\n",
    "#     # Keep the epochs low for submitting\n",
    "#     'epochs':      3\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Run the learning_rates sweep \n",
    "# for lr in learning_rates:\n",
    "\n",
    "#     model     = SimpleSentimentModel(\n",
    "#                     input_size=300,\n",
    "#                     hidden_size=base_hparams['hidden_size'],\n",
    "#                     dropout=base_hparams['dropout']\n",
    "#                ).to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     # Train and evaluate\n",
    "#     train_losses, val_losses, val_accs = train_and_evaluate(\n",
    "#         model, optimizer, criterion,\n",
    "#         train_loader, val_loader,\n",
    "#         base_hparams['epochs']\n",
    "#     )\n",
    "\n",
    "#     # Plot learning curves for this configuration\n",
    "#     plot_learning_curves(\n",
    "#         train_losses, \n",
    "#         val_losses, \n",
    "#         val_accs, \n",
    "#         prefix=f\"lr_{lr:.0e}\"\n",
    "#     )\n",
    "\n",
    "#     # Record final validation accuracy\n",
    "#     final_acc = val_accs[-1]\n",
    "#     print(f\"LR={lr:.0e} → Final Val Acc = {final_acc:.2f}%\")\n",
    "#     results.append((lr, final_acc))\n",
    "\n",
    "# # Summarize results \n",
    "# print(\"\\nLearning rate sweep results:\")\n",
    "# for lr, acc in results:\n",
    "#     print(f\"  {lr:.0e}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab34be4e",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-05-05T22:50:14.077071Z",
     "iopub.status.busy": "2025-05-05T22:50:14.076726Z",
     "iopub.status.idle": "2025-05-05T22:50:14.081263Z",
     "shell.execute_reply": "2025-05-05T22:50:14.080544Z"
    },
    "papermill": {
     "duration": 10.80351,
     "end_time": "2025-05-05T22:50:14.082627",
     "exception": false,
     "start_time": "2025-05-05T22:50:03.279117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define the hidden size parameters ---\n",
    "# hidden_sizes = [64, 128, 256, 512]\n",
    "# base_hparams = {\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'dropout':       0.2,\n",
    "#     'batch_size':    64,\n",
    "#     # Keep the epochs low for submitting\n",
    "#     'epochs':        3\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Run the hidden size sweep \n",
    "# for h in hidden_sizes:\n",
    "\n",
    "#     model = SimpleSentimentModel(\n",
    "#         input_size=300,\n",
    "#         hidden_size=h,\n",
    "#         dropout=base_hparams['dropout']\n",
    "#     ).to(device)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=base_hparams['learning_rate'])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     # Train and evaluate\n",
    "#     train_losses, val_losses, val_accs = train_and_evaluate(\n",
    "#         model, optimizer, criterion,\n",
    "#         train_loader, val_loader,\n",
    "#         base_hparams['epochs']\n",
    "#     )\n",
    "\n",
    "#     # Plot learning curves for this configuration\n",
    "#     plot_learning_curves(\n",
    "#         train_losses,\n",
    "#         val_losses,\n",
    "#         val_accs,\n",
    "#         prefix=f\"hidden_{h}\"\n",
    "#     )\n",
    "\n",
    "#     # Record final validation accuracy\n",
    "#     final_acc = val_accs[-1]\n",
    "#     print(f\"Hidden={h} → Final Val Acc = {final_acc:.2f}%\")\n",
    "#     results.append((h, final_acc))\n",
    "\n",
    "# # Summarize results \n",
    "# print(\"\\nHidden size sweep results:\")\n",
    "# for h, acc in results:\n",
    "#     print(f\"  Hidden={h}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86301733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:50:35.781552Z",
     "iopub.status.busy": "2025-05-05T22:50:35.781163Z",
     "iopub.status.idle": "2025-05-05T22:50:35.786392Z",
     "shell.execute_reply": "2025-05-05T22:50:35.785486Z"
    },
    "papermill": {
     "duration": 10.779901,
     "end_time": "2025-05-05T22:50:35.788078",
     "exception": false,
     "start_time": "2025-05-05T22:50:25.008177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define the dropout rate parameters ---\n",
    "# dropout_rates = [0.0, 0.2, 0.4, 0.5]\n",
    "# base_hparams = {\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'hidden_size':   64,\n",
    "#     'batch_size':    64,\n",
    "#     # Keep the epochs low for submitting\n",
    "#     'epochs':        3\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Run the dropout sweep \n",
    "# for d in dropout_rates:\n",
    "\n",
    "#     model = SimpleSentimentModel(\n",
    "#         input_size=300,\n",
    "#         hidden_size=base_hparams['hidden_size'],\n",
    "#         dropout=d\n",
    "#     ).to(device)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=base_hparams['learning_rate'])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     # Train and evaluate\n",
    "#     train_losses, val_losses, val_accs = train_and_evaluate(\n",
    "#         model, optimizer, criterion,\n",
    "#         train_loader, val_loader,\n",
    "#         base_hparams['epochs']\n",
    "#     )\n",
    "\n",
    "#     # Plot learning curves for this configuration\n",
    "#     plot_learning_curves(\n",
    "#         train_losses,\n",
    "#         val_losses,\n",
    "#         val_accs,\n",
    "#         prefix=f\"dropout_{int(d * 100)}\"\n",
    "#     )\n",
    "\n",
    "#     # Record final validation accuracy\n",
    "#     final_acc = val_accs[-1]\n",
    "#     print(f\"Dropout={d:.1f} → Final Val Acc = {final_acc:.2f}%\")\n",
    "#     results.append((d, final_acc))\n",
    "\n",
    "# # Summarize results\n",
    "# print(\"\\nDropout sweep results:\")\n",
    "# for d, acc in results:\n",
    "#     print(f\"  Dropout={d:.1f}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "843b88b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:50:57.402211Z",
     "iopub.status.busy": "2025-05-05T22:50:57.401876Z",
     "iopub.status.idle": "2025-05-05T22:50:57.406736Z",
     "shell.execute_reply": "2025-05-05T22:50:57.405808Z"
    },
    "papermill": {
     "duration": 10.876337,
     "end_time": "2025-05-05T22:50:57.408408",
     "exception": false,
     "start_time": "2025-05-05T22:50:46.532071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define the batch size parameters ---\n",
    "# batch_sizes = [32, 64, 128, 256]\n",
    "# base_hparams = {\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'hidden_size':   64,\n",
    "#     'dropout':       0.2,\n",
    "#     'epochs':        30\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Run the batch size sweep \n",
    "# for batch_size in batch_sizes:\n",
    "    \n",
    "#     # Create data loaders with current batch size\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "#     model = SimpleSentimentModel(\n",
    "#         input_size=300,\n",
    "#         hidden_size=base_hparams['hidden_size'],\n",
    "#         dropout=base_hparams['dropout']\n",
    "#     ).to(device)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=base_hparams['learning_rate'])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     # Train and evaluate\n",
    "#     train_losses, val_losses, val_accs = train_and_evaluate(\n",
    "#         model, optimizer, criterion,\n",
    "#         train_loader, val_loader,\n",
    "#         base_hparams['epochs']\n",
    "#     )\n",
    "\n",
    "#     # Plot learning curves for this configuration\n",
    "#     plot_learning_curves(\n",
    "#         train_losses, \n",
    "#         val_losses, \n",
    "#         val_accs, \n",
    "#         prefix=f\"batch_{batch_size}\"\n",
    "#     )\n",
    "\n",
    "#     # Record final validation accuracy\n",
    "#     final_acc = val_accs[-1]\n",
    "#     print(f\"Batch={batch_size} → Final Val Acc = {final_acc:.2f}%\")\n",
    "#     results.append((batch_size, final_acc))\n",
    "\n",
    "# # Summarize results \n",
    "# print(\"\\nBatch size sweep results:\")\n",
    "# for batch_size, acc in results:\n",
    "#     print(f\"  Batch={batch_size}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa83b535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:51:19.063772Z",
     "iopub.status.busy": "2025-05-05T22:51:19.063254Z",
     "iopub.status.idle": "2025-05-05T22:51:19.081806Z",
     "shell.execute_reply": "2025-05-05T22:51:19.080852Z"
    },
    "papermill": {
     "duration": 10.699558,
     "end_time": "2025-05-05T22:51:19.083376",
     "exception": false,
     "start_time": "2025-05-05T22:51:08.383818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, epochs, patience):\n",
    "    \n",
    "    train_losses, val_losses, val_accs = [], [], []    \n",
    "    \n",
    "    # Early‐stopping settings\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_epoch = -1\n",
    "    epochs_without_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "    \n",
    "        for texts, labels in train_loader:\n",
    "            # Move data to device\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "    \n",
    "            # Forward pass: get raw logits\n",
    "            logits = model(texts)\n",
    "    \n",
    "            # Calculate loss on logits vs. labels\n",
    "            loss = criterion(logits.squeeze(), labels)\n",
    "    \n",
    "            # Backward pass: calculate gradients and update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "        # Compute and record average training loss\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                logits = model(texts).squeeze()\n",
    "                running_val_loss += criterion(logits, labels).item()\n",
    "    \n",
    "                probs = torch.sigmoid(logits)\n",
    "                predicted = (probs > 0.5).float()\n",
    "    \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "    \n",
    "        val_acc = 100 * correct / total\n",
    "        val_accs.append(val_acc)\n",
    "    \n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "            f\"Val Acc: {val_acc:.2f}%\"\n",
    "        )\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improve = 0\n",
    "        else:\n",
    "            epochs_without_improve += 1\n",
    "            if epochs_without_improve >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (no improvement in val loss for {patience} epochs).\")\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses, val_accs, best_model_state, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91c47e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:51:40.701642Z",
     "iopub.status.busy": "2025-05-05T22:51:40.700865Z",
     "iopub.status.idle": "2025-05-05T22:51:40.707154Z",
     "shell.execute_reply": "2025-05-05T22:51:40.706167Z"
    },
    "papermill": {
     "duration": 10.770448,
     "end_time": "2025-05-05T22:51:40.708480",
     "exception": false,
     "start_time": "2025-05-05T22:51:29.938032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleFFNN(nn.Module):\n",
    "    def __init__(self, embedding_dim=300, hidden_size=64, dropout=0.2):\n",
    "        super(SimpleFFNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce2449a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:52:02.327371Z",
     "iopub.status.busy": "2025-05-05T22:52:02.327069Z",
     "iopub.status.idle": "2025-05-05T22:54:57.900785Z",
     "shell.execute_reply": "2025-05-05T22:54:57.899825Z"
    },
    "papermill": {
     "duration": 186.441258,
     "end_time": "2025-05-05T22:54:57.902592",
     "exception": false,
     "start_time": "2025-05-05T22:51:51.461334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.6049, Val Loss: 0.5469, Val Acc: 72.80%\n",
      "Epoch 2/30, Train Loss: 0.5378, Val Loss: 0.5203, Val Acc: 74.21%\n",
      "Epoch 3/30, Train Loss: 0.5231, Val Loss: 0.5118, Val Acc: 74.75%\n",
      "Epoch 4/30, Train Loss: 0.5172, Val Loss: 0.5080, Val Acc: 74.91%\n",
      "Epoch 5/30, Train Loss: 0.5134, Val Loss: 0.5050, Val Acc: 75.08%\n",
      "Epoch 6/30, Train Loss: 0.5108, Val Loss: 0.5033, Val Acc: 75.18%\n",
      "Epoch 7/30, Train Loss: 0.5085, Val Loss: 0.5012, Val Acc: 75.29%\n",
      "Epoch 8/30, Train Loss: 0.5066, Val Loss: 0.4997, Val Acc: 75.40%\n",
      "Epoch 9/30, Train Loss: 0.5052, Val Loss: 0.4996, Val Acc: 75.48%\n",
      "Epoch 10/30, Train Loss: 0.5037, Val Loss: 0.4973, Val Acc: 75.60%\n",
      "Epoch 11/30, Train Loss: 0.5023, Val Loss: 0.4967, Val Acc: 75.68%\n",
      "Epoch 12/30, Train Loss: 0.5010, Val Loss: 0.4953, Val Acc: 75.75%\n",
      "Epoch 13/30, Train Loss: 0.5001, Val Loss: 0.4945, Val Acc: 75.81%\n",
      "Epoch 14/30, Train Loss: 0.4990, Val Loss: 0.4943, Val Acc: 75.85%\n",
      "Epoch 15/30, Train Loss: 0.4983, Val Loss: 0.4929, Val Acc: 75.93%\n",
      "Epoch 16/30, Train Loss: 0.4973, Val Loss: 0.4924, Val Acc: 76.09%\n",
      "Epoch 17/30, Train Loss: 0.4961, Val Loss: 0.4914, Val Acc: 76.07%\n",
      "Epoch 18/30, Train Loss: 0.4955, Val Loss: 0.4908, Val Acc: 76.20%\n",
      "Epoch 19/30, Train Loss: 0.4943, Val Loss: 0.4899, Val Acc: 76.26%\n",
      "Epoch 20/30, Train Loss: 0.4936, Val Loss: 0.4895, Val Acc: 76.23%\n",
      "Epoch 21/30, Train Loss: 0.4928, Val Loss: 0.4887, Val Acc: 76.28%\n",
      "Epoch 22/30, Train Loss: 0.4920, Val Loss: 0.4881, Val Acc: 76.30%\n",
      "Epoch 23/30, Train Loss: 0.4916, Val Loss: 0.4881, Val Acc: 76.44%\n",
      "Epoch 24/30, Train Loss: 0.4905, Val Loss: 0.4877, Val Acc: 76.51%\n",
      "Epoch 25/30, Train Loss: 0.4897, Val Loss: 0.4868, Val Acc: 76.47%\n",
      "Epoch 26/30, Train Loss: 0.4893, Val Loss: 0.4866, Val Acc: 76.43%\n",
      "Epoch 27/30, Train Loss: 0.4884, Val Loss: 0.4851, Val Acc: 76.54%\n",
      "Epoch 28/30, Train Loss: 0.4872, Val Loss: 0.4847, Val Acc: 76.71%\n",
      "Epoch 29/30, Train Loss: 0.4874, Val Loss: 0.4842, Val Acc: 76.71%\n",
      "Epoch 30/30, Train Loss: 0.4865, Val Loss: 0.4835, Val Acc: 76.63%\n",
      "\n",
      "Best validation epoch: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducibility \n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model \n",
    "model = SimpleFFNN(embedding_dim=300, hidden_size=64, dropout=0.2)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train with early stopping \n",
    "train_losses, val_losses, val_accs, best_model_state, best_epoch = train_and_evaluate(\n",
    "    model, optimizer, criterion, train_loader, val_loader,\n",
    "    epochs=30, patience=3\n",
    ")\n",
    "\n",
    "print(f\"\\nBest validation epoch: {best_epoch}\")\n",
    "model.load_state_dict(best_model_state)  # Restore best weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d89dc25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T22:55:19.671594Z",
     "iopub.status.busy": "2025-05-05T22:55:19.670627Z",
     "iopub.status.idle": "2025-05-05T22:55:19.879662Z",
     "shell.execute_reply": "2025-05-05T22:55:19.878689Z"
    },
    "papermill": {
     "duration": 11.030227,
     "end_time": "2025-05-05T22:55:19.881452",
     "exception": false,
     "start_time": "2025-05-05T22:55:08.851225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_inputs = torch.tensor(test_vectors, dtype=torch.float32)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "# Predict on test set and generate submission.csv\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (texts,) in test_loader: \n",
    "        texts = texts.to(device)\n",
    "        logits = model(texts).squeeze()\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "# Save submission\n",
    "submission_df = pd.DataFrame({\"ID\": range(len(all_preds)), \"Label\": all_preds})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11446837,
     "sourceId": 96317,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1414.109302,
   "end_time": "2025-05-05T22:55:33.375529",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T22:31:59.266227",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
